{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import os\n",
    "import h5py\n",
    "from typing import List, Tuple\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##########################################\n",
    "# ENTER DATASET DIRECTORY PATH HERE ######\n",
    "##########################################\n",
    "DATASET_PATH = 'test_dataset'\n",
    "\n",
    "##########################################\n",
    "# ENTER PREDICTIONS DIRECTORY PATH HERE ##\n",
    "##########################################\n",
    "PREDICTIONS_PATH = 'predictions'\n",
    "\n",
    "# path to model (.pth file)\n",
    "MODEL_PATH = 'models/DiceBCE/DiceBCE_w09_checkpoint.pth'\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, k_size=3, stride=1, padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv3d = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=k_size,\n",
    "                                stride=stride, padding=padding)\n",
    "        self.batch_norm = nn.BatchNorm3d(num_features=out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm(self.conv3d(x))\n",
    "        # x = self.conv3d(x)\n",
    "        x = F.elu(x)\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, model_depth=4, pool_size=2):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.root_feat_maps = 4\n",
    "        self.num_conv_blocks = 2\n",
    "        # self.module_list = nn.ModuleList()\n",
    "        self.module_dict = nn.ModuleDict()\n",
    "        for depth in range(model_depth):\n",
    "            feat_map_channels = 2 ** (depth + 1) * self.root_feat_maps\n",
    "            for i in range(self.num_conv_blocks):\n",
    "                # print(\"depth {}, conv {}\".format(depth, i))\n",
    "                # print(in_channels, feat_map_channels)\n",
    "                self.conv_block = ConvBlock(in_channels=in_channels, out_channels=feat_map_channels)\n",
    "                self.module_dict['conv_{}_{}'.format(depth, i)] = self.conv_block\n",
    "                in_channels, feat_map_channels = feat_map_channels, feat_map_channels * 2\n",
    "\n",
    "            if depth == model_depth - 1:\n",
    "                break\n",
    "            else:\n",
    "                self.pooling = nn.MaxPool3d(kernel_size=pool_size, stride=2, padding=0)\n",
    "                self.module_dict['max_pooling_{}'.format(depth)] = self.pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_sampling_features = []\n",
    "        for k, op in self.module_dict.items():\n",
    "            if k.startswith('conv'):\n",
    "                x = op(x)\n",
    "                # print(k, x.shape)\n",
    "                if k.endswith('1'):\n",
    "                    down_sampling_features.append(x)\n",
    "            elif k.startswith('max_pooling'):\n",
    "                x = op(x)\n",
    "                # print(k, x.shape)\n",
    "\n",
    "        return x, down_sampling_features\n",
    "\n",
    "class ConvTranspose(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, k_size=3, stride=2, padding=1, output_padding=1):\n",
    "        super(ConvTranspose, self).__init__()\n",
    "        self.conv3d_transpose = nn.ConvTranspose3d(in_channels=in_channels,\n",
    "                                                   out_channels=out_channels,\n",
    "                                                   kernel_size=k_size,\n",
    "                                                   stride=stride,\n",
    "                                                   padding=padding,\n",
    "                                                   output_padding=output_padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv3d_transpose(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, out_channels, model_depth=4):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.num_conv_blocks = 2\n",
    "        self.num_feat_maps = 4\n",
    "        # user nn.ModuleDict() to store ops\n",
    "        self.module_dict = nn.ModuleDict()\n",
    "\n",
    "        for depth in range(model_depth - 2, -1, -1):\n",
    "            # print(depth)\n",
    "            feat_map_channels = 2 ** (depth + 1) * self.num_feat_maps\n",
    "            # print(feat_map_channels * 4)\n",
    "            self.deconv = ConvTranspose(in_channels=feat_map_channels * 4, out_channels=feat_map_channels * 4)\n",
    "            self.module_dict['deconv_{}'.format(depth)] = self.deconv\n",
    "            for i in range(self.num_conv_blocks):\n",
    "                if i == 0:\n",
    "                    self.conv = ConvBlock(in_channels=feat_map_channels * 6, out_channels=feat_map_channels * 2)\n",
    "                    self.module_dict['conv_{}_{}'.format(depth, i)] = self.conv\n",
    "                else:\n",
    "                    self.conv = ConvBlock(in_channels=feat_map_channels * 2, out_channels=feat_map_channels * 2)\n",
    "                    self.module_dict['conv_{}_{}'.format(depth, i)] = self.conv\n",
    "            if depth == 0:\n",
    "                self.final_conv = ConvBlock(in_channels=feat_map_channels * 2, out_channels=out_channels)\n",
    "                self.module_dict['final_conv'] = self.final_conv\n",
    "\n",
    "    def forward(self, x, down_sampling_features):\n",
    "        \"\"\"\n",
    "        :param x: inputs\n",
    "        :param down_sampling_features: feature maps from encoder path\n",
    "        :return: output\n",
    "        \"\"\"\n",
    "        for k, op in self.module_dict.items():\n",
    "            if k.startswith('deconv'):\n",
    "                x = op(x)\n",
    "                x = torch.cat((down_sampling_features[int(k[-1])], x), dim=1)\n",
    "            elif k.startswith('conv'):\n",
    "                x = op(x)\n",
    "            else:\n",
    "                x = op(x)\n",
    "        return x\n",
    "\n",
    "class UnetModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, model_depth=4, final_activation='sigmoid'):\n",
    "        super(UnetModel, self).__init__()\n",
    "        self.encoder = EncoderBlock(in_channels=in_channels, model_depth=model_depth)\n",
    "        self.decoder = DecoderBlock(out_channels=out_channels, model_depth=model_depth)\n",
    "        if final_activation == 'sigmoid':\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "        else:\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, downsampling_features = self.encoder(x)\n",
    "        x = self.decoder(x, downsampling_features)\n",
    "        x = self.sigmoid(x)\n",
    "        # print(\"Final output shape: \", x.shape)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_data(path: str) -> Tuple[List, List]:\n",
    "    data = []\n",
    "    indices_to_path = []\n",
    "\n",
    "    for root, dirs, filenames in os.walk(path):\n",
    "        for file in filenames:\n",
    "            f_path = os.path.join(root, file)\n",
    "            indices_to_path.append(f_path)\n",
    "            data.append(h5py.File(f_path))\n",
    "\n",
    "    return data, indices_to_path\n",
    "\n",
    "def get_raws(data: List[h5py.File]) -> np.ndarray:\n",
    "    raws = np.zeros(shape=(len(data), *data[0]['raw'].shape), dtype=np.uint8)\n",
    "    for i in range(len(raws)):\n",
    "        raws[i] = data[i]['raw']\n",
    "\n",
    "    return raws\n",
    "\n",
    "\n",
    "def load_as_images(raw_3d: np.ndarray):\n",
    "    \"\"\"\n",
    "    From a given raw of shape (channels, width, height), return arrays of Image of shape (channels)\n",
    "\n",
    "    :param raw_3d: 3D scan\n",
    "    :return: array of images (the 2D slices)\n",
    "    \"\"\"\n",
    "    n_img, channels, width, height = raw_3d.shape\n",
    "    raws_img = np.empty(shape=(n_img, channels), dtype=object)\n",
    "    for i in range(n_img):\n",
    "        for ch in range(channels):\n",
    "            raws_img[i][ch] = Image.fromarray(raw_3d[i][ch], mode='L')\n",
    "\n",
    "    return raws_img\n",
    "\n",
    "\n",
    "def img_to_numpy(raws_img):\n",
    "    np_imgs = np.empty(shape=(len(raws_img), len(raws_img[0]), raws_img[0][0].size[0], raws_img[0][0].size[0]), dtype=np.float32)\n",
    "    for i in range(len(raws_img)):\n",
    "        for ch in range(len(raws_img[0])):\n",
    "            np_imgs[i][ch] = np.asarray(raws_img[i][ch], dtype=np.float32)\n",
    "\n",
    "    return np_imgs\n",
    "\n",
    "def crop(raws_img: np.ndarray, box_size: int):\n",
    "    \"\"\"\n",
    "    Crop (inplace) on an area of the image, centered at the middle\n",
    "\n",
    "    :param raws_img: the raw images\n",
    "    :param box_size: size (in pixels) of the square box centered at the middle, which defines the crop area\n",
    "    \"\"\"\n",
    "    w, h = raws_img[0, 0].size  # images are square, w == h\n",
    "    if box_size > w:\n",
    "        raise ValueError(f'Received zoom box size {box_size}, which is bigger than image size ({w})')\n",
    "\n",
    "    if box_size % 2 == 0:\n",
    "        _box_size = box_size + 1\n",
    "    else:\n",
    "        _box_size = box_size\n",
    "\n",
    "    img_center = (w // 2, h // 2)\n",
    "\n",
    "    # crop the image, then resize (zoom) to original size (w, h)\n",
    "    # define crop box\n",
    "    x_left, x_right = img_center[0] - box_size // 2, img_center[0] + box_size // 2\n",
    "    y_top, y_bottom = img_center[1] - box_size // 2, img_center[1] + box_size // 2\n",
    "\n",
    "    for i in range(len(raws_img)):\n",
    "        for channel in range(len(raws_img[0])):\n",
    "            raws_img[i][channel] = raws_img[i][channel].crop((x_left, y_top, x_right, y_bottom))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data, file_paths = load_data(DATASET_PATH)\n",
    "raws_img = load_as_images(get_raws(test_data))\n",
    "original_size = raws_img[0][0].size[0]\n",
    "\n",
    "# transform data according to the format it was trained on\n",
    "crop(raws_img, box_size=32)  # trained on 64 x 32 x 32\n",
    "pad = (original_size // 2) - (32 // 2)\n",
    "# get data batch in tensor format\n",
    "raws_tensor = torch.from_numpy(img_to_numpy(raws_img)).unsqueeze(dim=1).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = UnetModel(in_channels=1, out_channels=1, model_depth=3)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Generate predictions\n",
    "probas_tensor = model(raws_tensor)\n",
    "preds = (probas_tensor > 0.5).long()\n",
    "preds = preds.detach().cpu().numpy().squeeze()\n",
    "\n",
    "for i, path in enumerate(file_paths):\n",
    "    _path = Path(path)\n",
    "    scan_name = _path.stem\n",
    "    pred_name = 'pred_' + scan_name\n",
    "    save_path = Path(PREDICTIONS_PATH).joinpath(pred_name)\n",
    "    # pad array to undo crop\n",
    "    padded_pred = np.zeros_like(preds[i], shape=(preds[i].shape[0], original_size, original_size), dtype=np.float32)\n",
    "    for ch in range(padded_pred.shape[0]):\n",
    "        padded_pred[ch][pad:-pad, pad:-pad] = preds[i][ch]\n",
    "    np.save(str(save_path), padded_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = model.to(torch.device('cpu'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'models/DiceBCE/DiceBCE_cpu_w09_checkpoint.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
